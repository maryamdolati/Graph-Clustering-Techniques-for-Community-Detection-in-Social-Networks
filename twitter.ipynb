{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5069d27",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "ebaafbe1bb8242b685b7d9f660f46761",
            "d759e7853a184e689ea3ee52003a85be",
            "db07f660632c45f7b7b3e1dd15cafaf4"
          ]
        },
        "id": "a5069d27",
        "outputId": "171c0512-a936-47c6-b9de-8f4341e2512d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset sentiment140 (C:/Users/AI-BIO/.cache/huggingface/datasets/sentiment140/sentiment140/1.0.0/7fdc297b986cb1dad1197eae755ef4d204f77fb43ba2bb81cc2a51a7565de122)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ebaafbe1bb8242b685b7d9f660f46761",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d759e7853a184e689ea3ee52003a85be",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Computing transition probabilities:   0%|          | 0/5000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db07f660632c45f7b7b3e1dd15cafaf4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Computing transition probabilities:   0%|          | 0/5000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                         Modularity       NMI  Conductance  ARI\n",
            "KMeans                     0.004712  0.150436     0.495285  0.0\n",
            "Hierarchical Clustering    0.003794  0.148293     0.496117  0.0\n",
            "Spectral Clustering        0.004758  0.150422     0.495238  0.0\n",
            "Louvain Method             0.053583  0.389125     0.821852  0.0\n",
            "DeepWalk                   0.037148  0.149884     0.462617  0.0\n",
            "Node2Vec                   0.037966  0.150345     0.461966  0.0\n",
            "GCN                        0.004712  0.150436     0.495285  0.0\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering\n",
        "from sklearn.metrics import normalized_mutual_info_score as NMI, adjusted_rand_score as ARI\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.utils import from_networkx\n",
        "from community import community_louvain\n",
        "from node2vec import Node2Vec\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "dataset = load_dataset('sentiment140')\n",
        "\n",
        "\n",
        "tweets = dataset['train']['text'][:5000]\n",
        "sentiments = dataset['train']['sentiment'][:5000]\n",
        "\n",
        "\n",
        "G = nx.Graph()\n",
        "for i, tweet in enumerate(tweets):\n",
        "    G.add_node(i, text=tweet, sentiment=sentiments[i])\n",
        "\n",
        "\n",
        "for i in range(len(tweets)):\n",
        "    for j in range(i + 1, len(tweets)):\n",
        "        if np.random.rand() > 0.95:\n",
        "            G.add_edge(i, j)\n",
        "\n",
        "node_features = torch.randn(G.number_of_nodes(), 16)\n",
        "data = from_networkx(G)\n",
        "data.x = node_features\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, 16)\n",
        "        self.conv2 = GCNConv(16, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "def compute_node_embeddings(data, num_clusters):\n",
        "    model = GCN(data.x.shape[1], num_clusters)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    model.train()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        initial_embeddings = model(data.x, data.edge_index)\n",
        "        kmeans = KMeans(n_clusters=num_clusters).fit(initial_embeddings.detach().numpy())\n",
        "        labels = torch.tensor(kmeans.labels_, dtype=torch.long)\n",
        "\n",
        "    for epoch in range(200):\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index)\n",
        "        loss = criterion(out, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        embeddings = model(data.x, data.edge_index).numpy()\n",
        "    return embeddings\n",
        "\n",
        "def conductance(G, clusters):\n",
        "    total_cut = 0\n",
        "    for cluster in clusters:\n",
        "        cut = nx.algorithms.cuts.cut_size(G, cluster)\n",
        "        volume = sum(dict(G.degree(cluster)).values())\n",
        "        if volume == 0:\n",
        "            continue\n",
        "        total_cut += cut / volume\n",
        "    return total_cut / len(clusters)\n",
        "\n",
        "num_clusters = 2\n",
        "\n",
        "\n",
        "gcn_embeddings = compute_node_embeddings(data, num_clusters)\n",
        "\n",
        "\n",
        "kmeans = KMeans(n_clusters=num_clusters).fit(gcn_embeddings)\n",
        "hierarchical = AgglomerativeClustering(n_clusters=num_clusters).fit(gcn_embeddings)\n",
        "spectral = SpectralClustering(n_clusters=num_clusters, affinity='nearest_neighbors').fit(gcn_embeddings)\n",
        "louvain = community_louvain.best_partition(G)\n",
        "\n",
        "\n",
        "def deepwalk_embedding(G, dimensions=64, walk_length=30, num_walks=200, workers=4):\n",
        "    node2vec = Node2Vec(G, dimensions=dimensions, walk_length=walk_length, num_walks=num_walks, workers=workers)\n",
        "    model = node2vec.fit(window=10, min_count=1)\n",
        "    embeddings = np.array([model.wv[str(node)] for node in G.nodes()])\n",
        "    return embeddings\n",
        "\n",
        "deepwalk_embeddings = deepwalk_embedding(G)\n",
        "\n",
        "\n",
        "node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=200, workers=4).fit()\n",
        "node2vec_embeddings = np.array([node2vec.wv[str(node)] for node in G.nodes()])\n",
        "\n",
        "\n",
        "deepwalk_clusters = KMeans(n_clusters=num_clusters).fit_predict(deepwalk_embeddings)\n",
        "node2vec_clusters = KMeans(n_clusters=num_clusters).fit_predict(node2vec_embeddings)\n",
        "\n",
        "\n",
        "def compute_metrics(G, labels):\n",
        "    if G.number_of_edges() == 0:\n",
        "        return 0, 0, 0, 0  #\n",
        "\n",
        "    modularity = nx.algorithms.community.quality.modularity(G, [list(np.where(labels == c)[0]) for c in np.unique(labels)])\n",
        "    nmi = NMI(list(G.nodes), labels)\n",
        "    ari = ARI(list(G.nodes), labels)\n",
        "    conductance_value = conductance(G, [list(np.where(labels == c)[0]) for c in np.unique(labels)])\n",
        "    return modularity, nmi, conductance_value, ari\n",
        "\n",
        "metrics = {\n",
        "    'KMeans': compute_metrics(G, kmeans.labels_),\n",
        "    'Hierarchical Clustering': compute_metrics(G, hierarchical.labels_),\n",
        "    'Spectral Clustering': compute_metrics(G, spectral.labels_),\n",
        "    'Louvain Method': compute_metrics(G, np.array(list(louvain.values()))),\n",
        "    'DeepWalk': compute_metrics(G, deepwalk_clusters),\n",
        "    'Node2Vec': compute_metrics(G, node2vec_clusters),\n",
        "    'GCN': compute_metrics(G, kmeans.labels_)\n",
        "}\n",
        "\n",
        "\n",
        "df = pd.DataFrame(metrics, index=['Modularity', 'NMI', 'Conductance', 'ARI']).transpose()\n",
        "\n",
        "\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff486be3",
      "metadata": {
        "id": "ff486be3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}